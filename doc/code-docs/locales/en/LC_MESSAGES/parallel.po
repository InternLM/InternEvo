# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, InternLM Team
# This file is distributed under the same license as the InternLM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
msgid ""
msgstr ""
"Project-Id-Version: InternLM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-01-23 18:01+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: en\n"
"Language-Team: en <LL@li.org>\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../source/parallel.rst:2
msgid "并行训练"
msgstr "Parallel Training"

#: ../../source/parallel.rst:6
msgid ""
"InternEvo 支持张量并行、流水线并行、序列并行、数据并行和 ZeRO1.5 "
"等并行化训练策略。在初始化分布式环境时，我们需要指定张量并行大小、流水线并行大小、数据并行大小以及 ZeRO1.5 策略。"
msgstr ""
"InternEvo supports tensor parallel, pipeline parallel, sequence parallel, "
"data parallel, and ZeRO1.5 to parallelize the training pipeline. When "
"initializing the distributed environment, we need to specify tensor "
"parallel size, pipeline parallel size, data parallel size, and ZeRO1.5 "
"strategy."

#: ../../source/parallel.rst:8
msgid ""
"InternEvo 的并行设置由配置文件中的 ``parallel`` 字段指定，用户可以通过修改配置文件 `config file "
"<https://github.com/InternLM/InternEvo/blob/develop/configs/7B_sft.py>`_ "
"来更改并行配置。以下是一个并行训练配置示例："
msgstr ""
"The parallel setting of InternEvo is fully config-driven, and you can "
"change the parallelism by modifying `config file "
"<https://github.com/InternLM/InternEvo/blob/develop/configs/7B_sft.py>`_. An "
"exmaple parallel training configuration can be defined as follows:"

#: ../../source/parallel.rst:19
msgid "zero1：zero 并行策略，分如下三种情况，默认值为 -1"
msgstr ""
"zero1: zero parallel strategy, divided into the following three cases, "
"the default value is -1"

#: ../../source/parallel.rst:21
msgid "当 ``zero1 <= 0``，则 zero1 进程组的大小等于数据并行进程组的大小，因此优化器状态参数将在数据并行范围内分配"
msgstr ""
"When ``zero1 <= 0``, the size of the zero1 process group is equal to the "
"size of the data parallel process group, so the optimizer state "
"parameters will be split within the data parallel range."

#: ../../source/parallel.rst:22
msgid "当 ``zero1 == 1``，则不使用 zero1 ，所有数据并行组保留完整的优化器状态参数"
msgstr ""
"When ``zero1 == 1``, zero1 is not used, and all data parallel groups "
"retain the complete optimizer state parameters."

#: ../../source/parallel.rst:23
msgid ""
"当 ``zero1 > 1`` 且 ``zero1 <= data_parallel_world_size``，则 zero1 "
"进程组是数据并行进程组的子集"
msgstr ""
"When ``zero1 > 1`` and ``zero1 <= data_parallel_world_size``, the zero1 "
"process group is a subset of the data parallel process group."

#: ../../source/parallel.rst:25
msgid "tensor：张量并行大小，通常是每个节点的 GPU 数量，默认值为 1"
msgstr ""
"tensor: tensor parallel size, usually the number of GPUs per node, the "
"default value is 1"

#: ../../source/parallel.rst:26
msgid "pipeline：流水线并行策略"
msgstr "pipeline: pipeline parallel strategy"

#: ../../source/parallel.rst:28
msgid "size：流水线并行大小，默认值为 1"
msgstr "size: pipeline parallel size, the default value is 1"

#: ../../source/parallel.rst:29
msgid "interleaved_overlap：bool 类型，交错式调度时，开启或关闭通信优化，默认值为 False"
msgstr ""
"interleaved_overlap: bool type, when interleaved scheduling, enable or "
"disable communication optimization, the default value is False"

#: ../../source/parallel.rst:31
msgid "sequence_parallel：是否开启序列化并行，默认值为 False"
msgstr ""
"sequence_parallel: whether to enable sequence parallelism, the default "
"value is False"

#: ../../source/parallel.rst:33
msgid "注意：数据并行大小 = 总的 GPU 数目 / 流水线并行大小 / 张量并行大小"
msgstr ""
"Note: `Data parallel size = Total number of GPUs / Pipeline parallel size"
" / Tensor parallel size`"

#: ../../source/parallel.rst:36
msgid "张量并行"
msgstr "Tensor Parallel"

#: ../../source/parallel.rst:38
msgid ""
"InternEvo 的张量并行实现方案基于 `flash attention <https://github.com/Dao-AILab"
"/flash-attention>`_, 主要对 `attention "
"<https://github.com/InternLM/InternEvo/blob/develop/internlm/model/multi_head_attention.py>`_"
" 和 `linear "
"<https://github.com/InternLM/InternEvo/blob/develop/internlm/model/linear.py>`_"
" 这两个模块进行张量并行操作。"
msgstr ""
"The implementation of tensor parallel for InternEvo is based on `flash "
"attention <https://github.com/Dao-AILab/flash-attention>`_, which has "
"tensor parallel extensions to parallelize `attention "
"<https://github.com/InternLM/InternEvo/blob/develop/internlm/model/multi_head_attention.py>`_"
" and `linear "
"<https://github.com/InternLM/InternEvo/blob/develop/internlm/model/linear.py>`_"
" blocks in InternEvo. "

#: ../../source/parallel.rst:41
msgid "用户可通过配置文件中的 ``parallel.tensor`` 字段来设置张量并行大小。"
msgstr ""
"To use tensor parallel, you need to set the value of tensor parallel size"
" ``parallel.tensor`` in the config file, which is usually the number of "
"GPUs per node."

#: ../../source/parallel.rst:47
msgid "张量并行，采用自 `flash-attention <https://arxiv.org/pdf/2205.14135.pdf>`_"
msgstr ""
"Tensor parallel, adopted from `flash-attention "
"<https://arxiv.org/pdf/2205.14135.pdf>`_"

#: ../../source/parallel.rst:50
msgid "流水线并行"
msgstr "Pipeline Parallel"

#: ../../source/parallel.rst:52
msgid ""
"InternEvo 在流水线并行中使用 `1F1B <https://arxiv.org/pdf/2104.04473.pdf>`_ "
"（1F1B，一次前向传递后跟一次反向传递）策略。对于 1F1B 策略，有两种实现方式："
msgstr ""
"InternEvo uses `1F1B <https://arxiv.org/pdf/2104.04473.pdf>`_ (one forward"
" pass followed by one backward pass) for pipeline parallel. For 1F1B "
"strategy, there are two implementations:"

#: ../../source/parallel.rst:54
msgid "非交错调度器，内存高效。"
msgstr "non-interleaved scheduler, which is memory-efficient"

#: ../../source/parallel.rst:55
msgid "交错调度器，内存高效且时间高效（GPU空泡较少）。"
msgstr "interleaved scheduler, which is both memory-efficient and time-efficient."

#: ../../source/parallel.rst:61
msgid "1F1B 流水线并行调度器，采用自 `Megatron-LM <https://arxiv.org/pdf/2104.04473.pdf>`_"
msgstr ""
"Non-interleaved and interleaved scheduler for 1F1B pipeline parallelism, "
"adopted from `Megatron-LM <https://arxiv.org/pdf/2104.04473.pdf>`_"

#: ../../source/parallel.rst:64
msgid "非交错式流水线调度"
msgstr "scheduler for non-interleaved 1F1B strategy"

#: ../../source/parallel.rst:65
msgid "如果要使用非交错式调度, 需要设置 ``model.num_chunks = 1``。"
msgstr ""
"To use non-interleaved pipeline scheduler, users need to set "
"``model.num_chunks = 1`` in the config file."

#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler:1 of
msgid ""
"A helper schedule class for pipeline parallelism running environment. It "
"uses non-interleaved 1F1B strategy. Other properties are similar as "
":class:`NonPipelineSchedule`."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.InterleavedPipelineScheduler.forward_backward_step
#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler
#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler.forward_backward_step
#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler.pre_processing
#: internlm.solver.optimizer.hybrid_zero_optim.HybridZeroOptimizer.belongs_to_current_rank
#: internlm.solver.optimizer.hybrid_zero_optim.HybridZeroOptimizer.step
#: internlm.solver.optimizer.hybrid_zero_optim.HybridZeroOptimizer.zero_grad of
msgid "参数"
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler:5 of
msgid "The number of microbatches."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler:7 of
msgid "Type of data. torch.float by default."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler:9 of
msgid ""
"The post processing function which receives a micro batch of data, and it"
" will be executed in `load_micro_batch`."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler:12 of
msgid "Specified shape in pipeline communication."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler:14 of
msgid ""
"If set to `True`, communication will be reduced over pipeline when using "
"1D tensor parallelization."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler:16 of
msgid "List of scheduler hooks."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler.pre_processing:1
#: of
msgid "To perform actions before running the schedule."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler.pre_processing:3
#: of
msgid "InternLM engine for training and inference."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler.forward_backward_step:1
#: of
msgid ""
"Runs non-interleaved 1F1B schedule, with communication between pipeline "
"stages. Returns a tuple with losses if the last stage, an empty tuple "
"otherwise."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.InterleavedPipelineScheduler.forward_backward_step:4
#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler.forward_backward_step:4
#: of
msgid "Colossalai engine for training and inference."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.InterleavedPipelineScheduler.forward_backward_step:6
#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler.forward_backward_step:6
#: of
msgid ""
"Dataloader as the form of an iterator, obtained by calling "
"iter(dataloader)."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.InterleavedPipelineScheduler.forward_backward_step:8
#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler.forward_backward_step:8
#: of
msgid ""
"Whether run forward step only. Default is false. If true, no backward "
"will be run."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.InterleavedPipelineScheduler.forward_backward_step:10
#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler.forward_backward_step:10
#: of
msgid "Whether returns the loss value. Default is true."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.InterleavedPipelineScheduler.forward_backward_step:12
#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler.forward_backward_step:12
#: of
msgid "If False, the output and label won't be returned."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.InterleavedPipelineScheduler.forward_backward_step
#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler.forward_backward_step
#: internlm.solver.optimizer.hybrid_zero_optim.HybridZeroOptimizer.belongs_to_current_rank
#: internlm.solver.optimizer.hybrid_zero_optim.HybridZeroOptimizer.step of
msgid "返回"
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.InterleavedPipelineScheduler.forward_backward_step:15
#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler.forward_backward_step:15
#: of
msgid ""
"A tuple of (output, label, loss, moe_loss), loss and label could be None."
"     The loss would be returned only in the last stage. And the moe_loss "
"is accumulated from all stages."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.InterleavedPipelineScheduler.forward_backward_step:17
#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler.forward_backward_step:17
#: of
msgid "A tuple of (output, label, loss, moe_loss), loss and label could be None."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.InterleavedPipelineScheduler.forward_backward_step:18
#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler.forward_backward_step:18
#: of
msgid ""
"The loss would be returned only in the last stage. And the moe_loss is "
"accumulated from all stages."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.InterleavedPipelineScheduler.forward_backward_step
#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler.forward_backward_step
#: internlm.solver.optimizer.hybrid_zero_optim.HybridZeroOptimizer.belongs_to_current_rank
#: internlm.solver.optimizer.hybrid_zero_optim.HybridZeroOptimizer.step of
msgid "返回类型"
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.InterleavedPipelineScheduler.forward_backward_step:19
#: internlm.core.scheduler.pipeline_scheduler.PipelineScheduler.forward_backward_step:19
#: of
msgid "Tuple[:class:`torch.Tensor`]"
msgstr ""

#: ../../source/parallel.rst:71
msgid "交错式流水线调度"
msgstr "scheduler for interleaved 1F1B strategy"

#: ../../source/parallel.rst:72
msgid "如果要使用交错式调度, 需要设置 ``model.num_chunks > 1``。"
msgstr ""
"To use interleaved pipeline scheduler, users need to set "
"``model.num_chunks > 1`` in the config file."

#: internlm.core.scheduler.pipeline_scheduler.InterleavedPipelineScheduler:1 of
msgid "Interleaved Pipeline Scheduler."
msgstr ""

#: internlm.core.scheduler.pipeline_scheduler.InterleavedPipelineScheduler.forward_backward_step:1
#: of
msgid ""
"Run interleaved 1F1B schedule (model split into model chunks), with "
"communication between pipeline stages as needed."
msgstr ""

#: ../../source/parallel.rst:77
msgid "值得注意的是，在使用交错式流水线调度器时可启用通信优化功能，即在 1F1B 阶段启用异步通信，以充分利用上行/下行带宽并实现通信与计算重叠。"
msgstr ""
"Asynchronous communication will be enabled in 1F1B stage to make full use"
" of uplink/downlink bandwidth and achieve communication overlap. "

#: ../../source/parallel.rst:79
msgid ""
"用户需要在配置文件中设置 ``parallel.pipeline.interleaved_overlap = "
"True``。该功能启用后，将调用函数 "
"``InterleavedPipelineScheduler._run_1f1b_loop_with_overlap``，并创建 "
"``internlm.core.communication.AsynCommunicator`` 以管理异步通信。"
msgstr ""
"When ``parallel.pipeline.interleaved_overlap = True``, function "
"``InterleavedPipelineScheduler._run_1f1b_loop_with_overlap`` will be "
"called and ``internlm.core.communication.AsynCommunicator`` will be "
"created for managing async communication."

#: ../../source/parallel.rst:81
msgid "``1F1B-without-overlap`` 和 ``1F1B-with-overlap`` 的区别如下所示："
msgstr ""
"The difference between 1F1B stage without overlap and 1F1B stage with "
"overlap is shown as follows:"

#: ../../source/parallel.rst:102
msgid "序列并行"
msgstr "Sequence Parallel"

#: ../../source/parallel.rst:104
msgid ""
"序列并行是一种在不引入额外计算、通信和内存开销的情况下，减少层 ``layer_norm`` 和 ``dropout`` "
"操作中的激活值内存。InternEvo 中的序列并行实现基于 `flash attention <https://github.com/Dao-"
"AILab/flash-attention>`_。这个并行策略有助于降低模型的内存消耗，提高了模型在资源受限环境中的可扩展性。"
msgstr ""
"Sequence parallel is a technique to reduce activation memory in layer "
"norm and dropout without additional computation, communication or memory "
"overhead. The implementation of sequence parallel for InternEvo is based "
"on `flash attention <https://github.com/Dao-AILab/flash-attention>`_. "

#: ../../source/parallel.rst:106
msgid "如果要启用序列并行, 用户需要设置 ``parallel.sequence_parallel = True``。"
msgstr ""
"To enable sequence parallel, you need to set ``parallel.sequence_parallel"
" = True`` in the config file."

#: ../../source/parallel.rst:112
msgid "序列并行, 采用自 flash-attention"
msgstr "Sequence parallel, adopted from flash-attention"

#: ../../source/parallel.rst:115
msgid "数据并行"
msgstr "Data Parallel"

#: ../../source/parallel.rst:117
msgid "InternEvo 支持数据并行。数据并行大小为:"
msgstr "InternEvo supports data parallel. For data parallel:"

#: ../../source/parallel.rst:119
msgid ""
"`Data parallel size = Total number of GPUs / Pipeline parallel size / "
"Tensor parallel size`"
msgstr ""

#: ../../source/parallel.rst:122
msgid "ZeRO1.5"
msgstr ""

#: ../../source/parallel.rst:124
msgid ""
"ZeRO1.5 的实现使用了分层分片的概念，通过配置值 ``parallel.zero1`` "
"启用了本地节点内的分片。这个方法有助于有效管理和分配模型参数和梯度，以减少内存使用并提高训练效率。"
msgstr ""
"The implementation of ZeRO1.5 uses the concept of hierarchical sharding "
"via config value ``parallel.zero1``, which enables sharding within local "
"nodes."

#: ../../source/parallel.rst:126
msgid "当 ``parallel.zero1 <= 0``，则 zero1 进程组的大小等于数据并行进程组的大小，因此优化器状态参数将在数据并行范围内分配"
msgstr ""
"If ``parallel.zero1 <= 0``, the size of the zero process group is equal "
"to the size of the dp process group, so parameters will be divided within"
" the range of dp."

#: ../../source/parallel.rst:127
msgid "当 ``parallel.zero1 == 1``，则不使用 zero1 ，所有数据并行组保留完整的优化器状态参数"
msgstr ""
"If ``parallel.zero1 == 1``, zero is not used, and all dp groups retain "
"the full amount of model parameters."

#: ../../source/parallel.rst:128
msgid ""
"当 ``parallel.zero1 > 1`` 且 ``parallel.zero1 <= "
"data_parallel_world_size``，则 zero1 进程组是数据并行进程组的子集"
msgstr ""
"If ``parallel.zero1 > 1`` and ``parallel.zero1 <= dp world size``, the "
"world size of zero is a subset of dp world size. For smaller models, it "
"is usually a better choice to split the parameters within nodes with a "
"setting ``parallel.zero1 <= 8``."

#: ../../source/parallel.rst:130
msgid ""
"此外，用户可以在配置文件中通过 ``hybrid_zero_optimizer`` "
"字段启用优化器的通信优化功能，设置桶大小，以及梯度剪裁等参数。这些设置有助于优化训练过程中的通信和计算效率，以及梯度的处理方式。"
msgstr ""
"Furthermore, you can enable communication-computation overlap, set bucket"
" reduce size, gradient clipping parameters in the config file."

#: ../../source/parallel.rst:144
msgid "这里有两个值得关注的通信优化点："
msgstr "There are two communication optimizations worth paying attention to here:"

#: ../../source/parallel.rst:146
msgid ""
"overlap_sync_grad: 如果设置为 ``True``，则将训练的 ``backward pass`` 与梯度的 ``all-"
"reduce`` 通信重叠"
msgstr ""
"overlap_sync_grad: If set True, overlapping training backward pass with "
"gradients' all-reduce communication."

#: ../../source/parallel.rst:147
msgid ""
"overlap_sync_param: 如果设置为 ``True``，则将参数的 ``broadcast`` 通信与下一步的 ``forward "
"pass`` 进行重叠"
msgstr ""
"overlap_sync_param: If set True, overlapping parameters' broadcast "
"communication with next step's forward pass."

#: ../../source/parallel.rst:149
msgid "这些优化可以加速训练过程，提高训练效率。"
msgstr ""
"These optimizations can speed up the training process and improve "
"training efficiency."

#: internlm.solver.optimizer.hybrid_zero_optim.HybridZeroOptimizer:1 of
msgid "Hybrid Zero Optimizer."
msgstr ""

#: internlm.solver.optimizer.hybrid_zero_optim.HybridZeroOptimizer.belongs_to_current_rank:1
#: of
msgid ""
"Check whether a parameter is supposed to be updated by the process of the"
" current rank"
msgstr ""

#: internlm.solver.optimizer.hybrid_zero_optim.HybridZeroOptimizer.belongs_to_current_rank:3
#: of
msgid "A :class:`torch.Tensor` object"
msgstr ""

#: internlm.solver.optimizer.hybrid_zero_optim.HybridZeroOptimizer.belongs_to_current_rank:6
#: of
msgid ""
"True if the parameter should be updated by the current rank. Otherwise "
"false."
msgstr ""

#: internlm.solver.optimizer.hybrid_zero_optim.HybridZeroOptimizer.zero_grad:1
#: of
msgid ""
"Set parameter gradients to zero. If set_to_none = True, gradient will be "
"set to None to save memory."
msgstr ""

#: internlm.solver.optimizer.hybrid_zero_optim.HybridZeroOptimizer.zero_grad:4
#: of
msgid "Whether set the gradient to None. Default value is True."
msgstr ""

#: internlm.solver.optimizer.hybrid_zero_optim.HybridZeroOptimizer.step:1 of
msgid "Performs a single optimization step."
msgstr ""

#: internlm.solver.optimizer.hybrid_zero_optim.HybridZeroOptimizer.step:3 of
msgid "A closure that reevaluates the model and returns the loss."
msgstr ""

#: internlm.solver.optimizer.hybrid_zero_optim.HybridZeroOptimizer.step:7 of
msgid "Whether the gradient is success updated, and the gradient."
msgstr ""

#~ msgid "A tuple of (output, label, loss), loss and label could be None."
#~ msgstr ""

#~ msgid ""
#~ "A tuple of (output, label, loss), "
#~ "loss and label could be None.     "
#~ "The loss would be returned only in"
#~ " the last stage."
#~ msgstr ""

#~ msgid "The loss would be returned only in the last stage."
#~ msgstr ""

